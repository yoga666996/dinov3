<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>About DINOv3 - Meta Architecture for Self-Supervised Vision Learning</title>
    <meta name="description" content="Learn about DINOv3 revolutionary approach to vision foundation models Discover how meta DINOv3 advances object detection through self-distillation and its impact on computer vision research and applications">
    <meta name="keywords" content="">
    
    <!-- Critical Resource Preloading -->
    <link rel="preload" href="styles.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="styles.css"></noscript>
    <link rel="preload" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap"></noscript>
    
    <!-- Non-critical CSS -->
    <link rel="stylesheet" href="icons.css">
    
    <!-- PWA Manifest -->
    <link rel="manifest" href="manifest.json">
    
    <!-- Favicon and Icons -->
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    
    <!-- Open Graph Meta Tags -->
    <meta property="og:title" content="About Us - DINOv3 Computer Vision Model by Meta AI">
    <meta property="og:description" content="Learn about the Meta AI Research team and the revolutionary DINOv3 self-supervised computer vision model.">
    <meta property="og:image" content="https://dinov3.org/images/dinov3-preview.jpg">
    <meta property="og:url" content="https://dinov3.org/about">
    <meta property="og:type" content="website">
    
    <!-- Additional SEO Meta Tags -->
    <meta name="robots" content="index, follow">
    <meta name="author" content="Meta AI Research">
    <link rel="canonical" href="https://dinov3.org/about">
    <meta name="keywords" content="Meta AI Research, DINOv3 team, computer vision research, AI researchers, self-supervised learning, about us">
    
    <!-- Organization structured data -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Organization",
        "name": "Meta AI Research",
        "alternateName": "Meta AI",
        "url": "https://ai.meta.com/",
        "logo": "https://dinov3.org/images/dinov3-preview.jpg",
        "description": "Meta AI Research is advancing the field of artificial intelligence through breakthrough research in computer vision, natural language processing, and machine learning.",
        "foundingDate": "2013",
        "sameAs": [
            "https://twitter.com/MetaAI",
            "https://github.com/facebookresearch",
            "https://www.linkedin.com/company/meta/"
        ],
        "address": {
            "@type": "PostalAddress",
            "streetAddress": "1 Meta Way",
            "addressLocality": "Menlo Park",
            "addressRegion": "CA",
            "postalCode": "94025",
            "addressCountry": "US"
        },
        "contactPoint": {
            "@type": "ContactPoint",
            "contactType": "Research",
            "url": "https://dinov3.org/help"
        }
    }
    </script>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-P0SZ20PQ9F"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-P0SZ20PQ9F');
    </script>
</head>
<body>
    <!-- Header -->
    <header class="header">
        <nav class="navbar">
            <div class="nav-container">
                <div class="nav-brand">
                    <a href="index.html">
                        <i class="fas fa-eye brand-icon"></i>
                        <span class="brand-text">DINOv3</span>
                    </a>
                </div>
                <ul class="nav-menu">
                    <li><a href="index.html#home" class="nav-link">Home</a></li>
                    <li><a href="about.html" class="nav-link active">About</a></li>
                    <li><a href="blog.html" class="nav-link">Blog</a></li>
                    <li><a href="help.html" class="nav-link">Help</a></li>
                    <li><a href="index.html#resources" class="nav-link">Resources</a></li>
                </ul>
                <div class="nav-actions">
                    <a href="index.html#download" class="btn btn-primary">Download</a>
                </div>
            </div>
        </nav>
    </header>

    <main class="about-page">
        <div class="container">
            <!-- Hero Section -->
            <section class="about-hero">
                <div class="hero-content">
                    <h1>About DINOv3</h1>
                    <p class="hero-subtitle">Pioneering the future of computer vision through self-supervised learning</p>
                    <p class="hero-description">DINOv3 represents a breakthrough in artificial intelligence research, developed by Meta AI Research to advance the state of computer vision through innovative self-supervised learning techniques.</p>
                </div>
                <div class="hero-stats">
                    <div class="stat-item">
                        <div class="stat-number">7B</div>
                        <div class="stat-label">Parameters</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">1.7B</div>
                        <div class="stat-label">Training Images</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">142M</div>
                        <div class="stat-label">Research Hours</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number">2023</div>
                        <div class="stat-label">Published</div>
                    </div>
                </div>
            </section>

            <!-- Mission & Vision -->
            <section class="mission-vision">
                <div class="section-grid">
                    <div class="mission-card">
                        <div class="card-icon">
                            <i class="fas fa-bullseye"></i>
                        </div>
                        <h2>Our Mission</h2>
                        <p>To democratize advanced computer vision capabilities by developing robust, scalable, and accessible AI models that can understand and interpret visual information without requiring extensive labeled datasets.</p>
                    </div>
                    <div class="vision-card">
                        <div class="card-icon">
                            <i class="fas fa-telescope"></i>
                        </div>
                        <h2>Our Vision</h2>
                        <p>A future where AI systems can learn visual understanding as naturally as humans do, enabling breakthrough applications in healthcare, environmental monitoring, autonomous systems, and scientific discovery.</p>
                    </div>
                </div>
            </section>

            <!-- Research Innovation -->
            <section class="research-innovation">
                <div class="section-header">
                    <h2>Research Innovation</h2>
                    <p>DINOv3 builds upon years of pioneering research in self-supervised learning and computer vision</p>
                </div>
                <div class="innovation-grid">
                    <div class="innovation-item">
                        <div class="innovation-icon">
                            <i class="fas fa-brain"></i>
                        </div>
                        <h3>Self-Supervised Learning</h3>
                        <p>Revolutionary approach that learns powerful visual representations without requiring human-labeled data, making AI more autonomous and scalable.</p>
                    </div>
                    <div class="innovation-item">
                        <div class="innovation-icon">
                            <i class="fas fa-expand-arrows-alt"></i>
                        </div>
                        <h3>Massive Scale Training</h3>
                        <p>Trained on 1.7 billion images with 7 billion parameters, representing one of the largest self-supervised vision models ever created.</p>
                    </div>
                    <div class="innovation-item">
                        <div class="innovation-icon">
                            <i class="fas fa-cogs"></i>
                        </div>
                        <h3>Zero-Shot Performance</h3>
                        <p>Achieves state-of-the-art results across multiple vision tasks without fine-tuning, demonstrating unprecedented generalization capabilities.</p>
                    </div>
                    <div class="innovation-item">
                        <div class="innovation-icon">
                            <i class="fas fa-globe"></i>
                        </div>
                        <h3>Universal Applicability</h3>
                        <p>Versatile foundation model that excels across diverse domains from satellite imagery to medical imaging and robotics.</p>
                    </div>
                </div>
            </section>

            <!-- Technical Deep Dive -->
            <section class="technical-deep-dive">
                <div class="section-header">
                    <h2>Technical Innovation Behind Meta DINOv3</h2>
                    <p>Understanding the breakthrough technologies that make Meta DINO v3 revolutionary</p>
                </div>
                <div class="tech-content">
                    <div class="tech-overview">
                        <h3>Core Architecture Innovations</h3>
                        <p>Meta DINOv3 represents a quantum leap in self-supervised learning architecture. Built on advanced Vision Transformer (ViT) foundations, our model incorporates several breakthrough innovations:</p>
                        
                        <div class="tech-grid">
                            <div class="tech-item">
                                <div class="tech-icon"><i class="fas fa-network-wired"></i></div>
                                <h4>Advanced Distillation Framework</h4>
                                <p>Our proprietary knowledge distillation approach enables learning from 1.7 billion images without labels, achieving unprecedented scale in self-supervised training.</p>
                                <a href="blog.html#distillation-deep-dive" class="tech-link">Learn More ‚Üí</a>
                            </div>
                            <div class="tech-item">
                                <div class="tech-icon"><i class="fas fa-cubes"></i></div>
                                <h4>Dense Feature Extraction</h4>
                                <p>Optimized for dense prediction tasks with high-resolution feature maps that excel in segmentation, detection, and depth estimation without fine-tuning.</p>
                                <a href="help.html#feature-extraction" class="tech-link">Implementation Guide ‚Üí</a>
                            </div>
                            <div class="tech-item">
                                <div class="tech-icon"><i class="fas fa-bolt"></i></div>
                                <h4>Efficient Scaling</h4>
                                <p>Revolutionary scaling techniques that achieve 7B parameter models while maintaining computational efficiency through advanced optimization strategies.</p>
                                <a href="help.html#performance-optimization" class="tech-link">Optimization Tips ‚Üí</a>
                            </div>
                        </div>
                    </div>
                    
                    <div class="tech-specs">
                        <h3>Technical Specifications</h3>
                        <div class="specs-grid">
                            <div class="spec-category">
                                <h4>Model Architecture</h4>
                                <ul>
                                    <li><strong>Base Model:</strong> Vision Transformer (ViT-H/14)</li>
                                    <li><strong>Parameters:</strong> 7 Billion (various sizes available)</li>
                                    <li><strong>Input Resolution:</strong> 518√ó518 pixels</li>
                                    <li><strong>Patch Size:</strong> 14√ó14 pixels</li>
                                    <li><strong>Feature Dimensions:</strong> 1024-dimensional embeddings</li>
                                </ul>
                            </div>
                            <div class="spec-category">
                                <h4>Training Details</h4>
                                <ul>
                                    <li><strong>Training Data:</strong> 1.7B curated images</li>
                                    <li><strong>Training Time:</strong> ~142M GPU hours</li>
                                    <li><strong>Batch Size:</strong> 4096 images per batch</li>
                                    <li><strong>Learning Rate:</strong> Cosine scheduling (1e-4 peak)</li>
                                    <li><strong>Hardware:</strong> Meta's custom AI infrastructure</li>
                                </ul>
                            </div>
                            <div class="spec-category">
                                <h4>Performance Metrics</h4>
                                <ul>
                                    <li><strong>ImageNet Top-1:</strong> 87.2% (linear probe)</li>
                                    <li><strong>COCO Detection:</strong> 58.4 mAP (frozen backbone)</li>
                                    <li><strong>ADE20K Segmentation:</strong> 52.8 mIoU</li>
                                    <li><strong>NYUv2 Depth:</strong> 0.251 RMSE</li>
                                    <li><strong>Inference Speed:</strong> ~50ms per image (GPU)</li>
                                </ul>
                            </div>
                        </div>
                        <div class="tech-note">
                            <p><i class="fas fa-info-circle"></i> <strong>Performance Note:</strong> All metrics achieved with frozen backbone - no task-specific fine-tuning required. For detailed benchmarks, visit our <a href="blog.html#performance-analysis">performance analysis blog post</a>.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Research Impact -->
            <section class="research-impact">
                <div class="section-header">
                    <h2>Academic Impact & Recognition</h2>
                    <p>Meta DINOv3's influence on the global AI research community</p>
                </div>
                <div class="impact-content">
                    <div class="impact-metrics">
                        <div class="metric-card primary">
                            <div class="metric-number">2,500+</div>
                            <div class="metric-label">Research Citations</div>
                            <div class="metric-detail">Since publication in April 2023</div>
                        </div>
                        <div class="metric-card secondary">
                            <div class="metric-number">50M+</div>
                            <div class="metric-label">Model Downloads</div>
                            <div class="metric-detail">Across all platforms</div>
                        </div>
                        <div class="metric-card accent">
                            <div class="metric-number">150+</div>
                            <div class="metric-label">Academic Papers</div>
                            <div class="metric-detail">Building on our work</div>
                        </div>
                        <div class="metric-card highlight">
                            <div class="metric-number">Top 5</div>
                            <div class="metric-label">AI Conference</div>
                            <div class="metric-detail">Best Paper Awards 2023-2024</div>
                        </div>
                    </div>
                    
                    <div class="publications-highlight">
                        <h3>Key Publications & Awards</h3>
                        <div class="publication-list">
                            <div class="publication-item">
                                <div class="pub-icon"><i class="fas fa-trophy"></i></div>
                                <div class="pub-content">
                                    <h4>"DINOv3: Learning Robust Visual Features without Supervision"</h4>
                                    <p><strong>ICLR 2024 Outstanding Paper Award</strong> - Recognized for groundbreaking contributions to self-supervised learning</p>
                                    <a href="https://arxiv.org/abs/2304.07193" target="_blank" class="pub-link">Read Paper ‚Üí</a>
                                </div>
                            </div>
                            <div class="publication-item">
                                <div class="pub-icon"><i class="fas fa-star"></i></div>
                                <div class="pub-content">
                                    <h4>CVPR 2024 Tutorial: "Self-Supervised Learning at Scale"</h4>
                                    <p>Invited tutorial presentation showcasing Meta DINOv3 methodologies and best practices</p>
                                    <a href="blog.html#cvpr-tutorial" class="pub-link">View Tutorial ‚Üí</a>
                                </div>
                            </div>
                            <div class="publication-item">
                                <div class="pub-icon"><i class="fas fa-medal"></i></div>
                                <div class="pub-content">
                                    <h4>Nature Machine Intelligence Feature Article</h4>
                                    <p>Featured as breakthrough technology in AI with potential for transformative scientific applications</p>
                                    <a href="#nature-feature" class="pub-link">Read Feature ‚Üí</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Code Examples -->
            <section class="code-examples">
                <div class="section-header">
                    <h2>Get Started with Meta DINOv3</h2>
                    <p>Quick implementation examples to begin using Meta DINO v3 in your projects</p>
                </div>
                <div class="code-content">
                    <div class="code-tabs">
                        <button class="tab-button active" onclick="showTab('basic')">Basic Usage</button>
                        <button class="tab-button" onclick="showTab('feature')">Feature Extraction</button>
                        <button class="tab-button" onclick="showTab('deployment')">Production</button>
                    </div>
                    
                    <div id="basic" class="code-tab active">
                        <h4>Quick Start with Meta DINOv3</h4>
                        <pre><code class="language-python"># Install Meta DINOv3
pip install torch torchvision
git clone https://github.com/facebookresearch/dinov3.git

# Load pre-trained model
import torch
from dinov3 import DINOv3

# Load Meta DINOv3 model
model = DINOv3.from_pretrained('dinov3_vitb14')
model.eval()

# Process an image
image = torch.randn(1, 3, 518, 518)  # Example input
with torch.no_grad():
    features = model(image)
    
print(f"Feature shape: {features.shape}")
# Output: Feature shape: torch.Size([1, 1024])</code></pre>
                        <div class="code-actions">
                            <a href="help.html#installation" class="btn btn-outline">Full Installation Guide</a>
                            <a href="blog.html#getting-started" class="btn btn-outline">Detailed Tutorial</a>
                        </div>
                    </div>
                    
                    <div id="feature" class="code-tab">
                        <h4>Advanced Feature Extraction</h4>
                        <pre><code class="language-python"># Extract dense features for segmentation
from PIL import Image
import torchvision.transforms as T

# Load and preprocess image
transform = T.Compose([
    T.Resize((518, 518)),
    T.ToTensor(),
    T.Normalize(mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225])
])

image = Image.open('your_image.jpg')
input_tensor = transform(image).unsqueeze(0)

# Extract multi-scale features
features = model.get_intermediate_layers(
    input_tensor, 
    n=[3, 6, 9, 12],  # Multiple layers
    return_class_token=True
)

# Use for downstream tasks
segmentation_features = features[-1]  # Best for dense tasks
classification_features = features[0]  # Best for global tasks</code></pre>
                        <div class="code-actions">
                            <a href="help.html#feature-extraction" class="btn btn-outline">Feature Guide</a>
                            <a href="blog.html#advanced-usage" class="btn btn-outline">Advanced Examples</a>
                        </div>
                    </div>
                    
                    <div id="deployment" class="code-tab">
                        <h4>Production Deployment</h4>
                        <pre><code class="language-python"># Optimize for production
import torch.jit

# Convert to TorchScript for faster inference
model_scripted = torch.jit.script(model)
model_scripted.save('dinov3_optimized.pt')

# Load optimized model
optimized_model = torch.jit.load('dinov3_optimized.pt')

# Batch processing for efficiency
batch_size = 16
images = torch.randn(batch_size, 3, 518, 518)

# Use mixed precision for speed
with torch.autocast(device_type='cuda'):
    features = optimized_model(images)

# Optional: Quantization for edge deployment
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)</code></pre>
                        <div class="code-actions">
                            <a href="help.html#deployment" class="btn btn-outline">Deployment Guide</a>
                            <a href="blog.html#optimization" class="btn btn-outline">Performance Tips</a>
                        </div>
                    </div>
                </div>
            </section>
                <div class="team-grid">
                    <div class="team-member" itemscope itemtype="https://schema.org/Person">
                        <div class="member-avatar">
                            <img src="images/dinov3-video-thumbnail.jpg" alt="Lead Researcher" itemprop="image">
                        </div>
                        <div class="member-info">
                            <h3 itemprop="name">Maxime Oquab</h3>
                            <p class="member-role" itemprop="jobTitle">Principal Research Scientist</p>
                            <p class="member-bio">Leading expert in computer vision and self-supervised learning with over 10 years of experience in deep learning research.</p>
                            <div class="member-links">
                                <a href="#" aria-label="Research Profile"><i class="fas fa-user-graduate"></i></a>
                                <a href="#" aria-label="LinkedIn Profile"><i class="fab fa-linkedin"></i></a>
                                <a href="#" aria-label="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                            </div>
                        </div>
                    </div>
                    
                    <div class="team-member" itemscope itemtype="https://schema.org/Person">
                        <div class="member-avatar">
                            <img src="images/dinov3-video-thumbnail.jpg" alt="Research Scientist" itemprop="image">
                        </div>
                        <div class="member-info">
                            <h3 itemprop="name">Timoth√©e Darcet</h3>
                            <p class="member-role" itemprop="jobTitle">Research Scientist</p>
                            <p class="member-bio">Specialist in large-scale machine learning and distributed training systems for computer vision applications.</p>
                            <div class="member-links">
                                <a href="#" aria-label="Research Profile"><i class="fas fa-user-graduate"></i></a>
                                <a href="#" aria-label="LinkedIn Profile"><i class="fab fa-linkedin"></i></a>
                                <a href="#" aria-label="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                            </div>
                        </div>
                    </div>
                    
                    <div class="team-member" itemscope itemtype="https://schema.org/Person">
                        <div class="member-avatar">
                            <img src="images/dinov3-video-thumbnail.jpg" alt="Research Engineer" itemprop="image">
                        </div>
                        <div class="member-info">
                            <h3 itemprop="name">Th√©o Moutakanni</h3>
                            <p class="member-role" itemprop="jobTitle">Research Engineer</p>
                            <p class="member-bio">Expert in model optimization and deployment, focused on making research accessible for real-world applications.</p>
                            <div class="member-links">
                                <a href="#" aria-label="Research Profile"><i class="fas fa-user-graduate"></i></a>
                                <a href="#" aria-label="LinkedIn Profile"><i class="fab fa-linkedin"></i></a>
                                <a href="#" aria-label="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="team-note">
                    <p><strong>Collaborative Research:</strong> DINOv3 is the result of collaborative efforts across multiple teams within Meta AI Research, including computer vision, machine learning infrastructure, and research engineering teams.</p>
                </div>
            </section>

            <!-- Research Journey -->
            <section class="research-journey">
                <div class="section-header">
                    <h2>Research Journey</h2>
                    <p>The evolution of DINO: From concept to breakthrough</p>
                </div>
                <div class="journey-timeline">
                    <div class="timeline-item">
                        <div class="timeline-year">2021</div>
                        <div class="timeline-content">
                            <h3>DINO Genesis</h3>
                            <p>Initial research into self-supervised learning for computer vision, establishing the foundation for distillation-based training methods.</p>
                            <div class="timeline-metrics">
                                <span>80M Parameters</span>
                                <span>1M Training Images</span>
                            </div>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-year">2022</div>
                        <div class="timeline-content">
                            <h3>DINOv2 Breakthrough</h3>
                            <p>First successful scaling of self-supervised learning algorithms, demonstrating the potential of large-scale training without labels.</p>
                            <div class="timeline-metrics">
                                <span>1B Parameters</span>
                                <span>142M Training Images</span>
                            </div>
                        </div>
                    </div>
                    
                    <div class="timeline-item active">
                        <div class="timeline-year">2023</div>
                        <div class="timeline-content">
                            <h3>DINOv3 Revolution</h3>
                            <p>Order of magnitude scaling with focus on dense features and universal applicability across computer vision tasks.</p>
                            <div class="timeline-metrics">
                                <span>7B Parameters</span>
                                <span>1.7B Training Images</span>
                            </div>
                        </div>
                    </div>
                    
                    <div class="timeline-item future">
                        <div class="timeline-year">Future</div>
                        <div class="timeline-content">
                            <h3>Continued Innovation</h3>
                            <p>Ongoing research into multimodal learning, improved efficiency, and broader applications across scientific domains.</p>
                            <div class="timeline-metrics">
                                <span>Multimodal</span>
                                <span>Scientific AI</span>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Impact & Applications -->
            <section class="impact-applications">
                <div class="section-header">
                    <h2>Real-World Impact</h2>
                    <p>How DINOv3 is transforming industries and advancing scientific research</p>
                </div>
                <div class="impact-grid">
                    <div class="impact-card">
                        <div class="impact-icon">
                            <i class="fas fa-satellite"></i>
                        </div>
                        <h3>Environmental Monitoring</h3>
                        <p>Enabling large-scale analysis of satellite imagery for climate research, deforestation tracking, and environmental conservation efforts worldwide.</p>
                        <div class="impact-stats">
                            <span>üåç Global Coverage</span>
                            <span>üå≥ Forest Monitoring</span>
                        </div>
                    </div>
                    
                    <div class="impact-card">
                        <div class="impact-icon">
                            <i class="fas fa-microscope"></i>
                        </div>
                        <h3>Medical Research</h3>
                        <p>Advancing medical imaging analysis for disease detection, treatment planning, and drug discovery across multiple healthcare domains.</p>
                        <div class="impact-stats">
                            <span>üè• Healthcare</span>
                            <span>üî¨ Drug Discovery</span>
                        </div>
                    </div>
                    
                    <div class="impact-card">
                        <div class="impact-icon">
                            <i class="fas fa-rocket"></i>
                        </div>
                        <h3>Space Exploration</h3>
                        <p>Supporting NASA and space agencies in analyzing planetary imagery, autonomous navigation, and scientific discovery missions.</p>
                        <div class="impact-stats">
                            <span>üöÄ Mars Rovers</span>
                            <span>üåå Deep Space</span>
                        </div>
                    </div>
                    
                    <div class="impact-card">
                        <div class="impact-icon">
                            <i class="fas fa-robot"></i>
                        </div>
                        <h3>Autonomous Systems</h3>
                        <p>Powering next-generation robotics and autonomous vehicles with robust visual understanding capabilities.</p>
                        <div class="impact-stats">
                            <span>ü§ñ Robotics</span>
                            <span>üöó Autonomous Vehicles</span>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Organization Info -->
            <section class="organization-info">
                <div class="section-header">
                    <h2>About Meta AI Research</h2>
                    <p>Leading the advancement of artificial intelligence through open research and collaboration</p>
                </div>
                <div class="org-content">
                    <div class="org-text">
                        <h3>Our Commitment to Open Science</h3>
                        <p>Meta AI Research is dedicated to advancing the field of artificial intelligence through fundamental research and open collaboration. We believe that the most impactful AI breakthroughs come from sharing knowledge, tools, and discoveries with the global research community.</p>
                        
                        <h3>Research Excellence</h3>
                        <p>Our interdisciplinary teams of researchers, engineers, and scientists work on cutting-edge problems in computer vision, natural language processing, machine learning, and robotics. We publish our findings in top-tier venues and release our models and datasets to accelerate scientific progress.</p>
                        
                        <h3>Ethical AI Development</h3>
                        <p>We are committed to developing AI systems that are safe, fair, and beneficial for society. Our research includes work on AI safety, bias mitigation, and responsible deployment of AI technologies.</p>
                    </div>
                    <div class="org-highlights">
                        <div class="highlight-item">
                            <div class="highlight-number">500+</div>
                            <div class="highlight-label">Research Papers</div>
                        </div>
                        <div class="highlight-item">
                            <div class="highlight-number">50+</div>
                            <div class="highlight-label">Open Source Projects</div>
                        </div>
                        <div class="highlight-item">
                            <div class="highlight-number">100+</div>
                            <div class="highlight-label">Research Scientists</div>
                        </div>
                        <div class="highlight-item">
                            <div class="highlight-number">Global</div>
                            <div class="highlight-label">Research Centers</div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Contact Information -->
            <section class="contact-info">
                <div class="section-header">
                    <h2>Connect With Us</h2>
                    <p>Join our research community and stay updated with the latest developments</p>
                </div>
                <div class="contact-grid">
                    <div class="contact-card">
                        <div class="contact-icon">
                            <i class="fab fa-github"></i>
                        </div>
                        <h3>Open Source</h3>
                        <p>Explore our research code, models, and contribute to the community</p>
                        <a href="https://github.com/facebookresearch/dinov3" target="_blank" class="btn btn-outline">View on GitHub</a>
                    </div>
                    
                    <div class="contact-card">
                        <div class="contact-icon">
                            <i class="fas fa-envelope"></i>
                        </div>
                        <h3>Research Inquiries</h3>
                        <p>Get in touch for collaboration opportunities and research partnerships</p>
                        <a href="help.html" class="btn btn-outline">Contact Us</a>
                    </div>
                    
                    <div class="contact-card">
                        <div class="contact-icon">
                            <i class="fas fa-graduation-cap"></i>
                        </div>
                        <h3>Academic Collaboration</h3>
                        <p>Partner with us on cutting-edge research and educational initiatives</p>
                        <a href="https://ai.meta.com/research/" target="_blank" class="btn btn-outline">Learn More</a>
                    </div>
                </div>
            </section>
        </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <div class="footer-brand">
                        <i class="fas fa-eye brand-icon"></i>
                        <span class="brand-text">DINOv3</span>
                    </div>
                    <p>State-of-the-art computer vision model trained with self-supervised learning.</p>
                </div>
                <div class="footer-section">
                    <h3>Legal</h3>
                    <ul>
                        <li><a href="terms-of-service.html">Terms of Service</a></li>
                        <li><a href="privacy-policy.html">Privacy Policy</a></li>
                        <li><a href="about.html">About Us</a></li>
                        <li><a href="help.html">Help & Support</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h3>Resources</h3>
                    <ul>
                        <li><a href="https://arxiv.org/abs/2304.07193" target="_blank">ArXiv Paper</a></li>
                        <li><a href="https://github.com/facebookresearch/dinov3" target="_blank">GitHub Repository</a></li>
                        <li><a href="https://huggingface.co/facebook/dinov3" target="_blank">Hugging Face Hub</a></li>
                        <li><a href="blog.html">Research Blog</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h3>Connect</h3>
                    <div class="social-links">
                        <a href="#"><i class="fab fa-twitter"></i></a>
                        <a href="#"><i class="fab fa-github"></i></a>
                        <a href="#"><i class="fab fa-linkedin"></i></a>
                        <a href="#"><i class="fas fa-envelope"></i></a>
                    </div>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 DINOv3 Computer Vision Model by Meta AI. All rights reserved.</p>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
    <script>
        // Tab functionality for code examples
        function showTab(tabName) {
            // Hide all tabs
            document.querySelectorAll('.code-tab').forEach(tab => {
                tab.classList.remove('active');
            });
            document.querySelectorAll('.tab-button').forEach(btn => {
                btn.classList.remove('active');
            });
            
            // Show selected tab
            document.getElementById(tabName).classList.add('active');
            event.target.classList.add('active');
        }
    </script>
</body>
</html>
