<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DINOv3: Meta AI Computer Vision Model | Tutorial, PyTorch Implementation & Demo</title>
    <meta name="description" content="Official DINOv3 by Meta AI - Advanced computer vision model with PyTorch implementation. Complete tutorials, demos, OCR applications, and segmentation guides. Download pre-trained models.">
    <meta name="keywords" content="dinov3, dino v3, dinov3 tutorial, dinov3 pytorch, dinov3 demo, dinov3 ocr, dinov3 segmentation, meta ai, computer vision, vision transformer, self-supervised learning">

    <meta property="og:title" content="DINOv3: Meta AI Computer Vision Model | Official Tutorial & Implementation">
    <meta property="og:description" content="Learn DINOv3 - Meta AI's advanced computer vision model. Complete PyTorch tutorials, demos, OCR applications, and segmentation examples.">
    <meta property="og:url" content="https://dinov3.org/">
    <meta property="og:type" content="website">
    <meta property="og:image" content="https://dinov3.org/images/dinov3-video-thumbnail.webp">
    
    <!-- Twitter Cards -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="DINOv3: Meta AI Computer Vision Model">
    <meta name="twitter:description" content="Complete DINOv3 tutorial with PyTorch implementation, demos, and applications.">
    <meta name="twitter:image" content="https://dinov3.org/images/dinov3-video-thumbnail.webp">
    <meta name="google-site-verification" content="your-google-verification-code">
    <meta name="msvalidate.01" content="your-bing-verification-code">
    <meta name="yandex-verification" content="your-yandex-verification-code">
    <meta name="monetag" content="dcfcd9e07cfc3843b6a870e571406790">

    <!-- Optimized browser caching strategy -->
    <meta http-equiv="Cache-Control" content="public, max-age=86400, stale-while-revalidate=604800">
    <meta http-equiv="Expires" content="Thu, 31 Dec 2025 23:59:59 GMT">
    
    <!-- Performance monitoring and Core Web Vitals -->
    <meta name="theme-color" content="#ff6b35">
    <meta name="color-scheme" content="dark">
    
    <!-- Performance Optimization - Preconnect and DNS prefetch -->
    <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
    <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
    <link rel="dns-prefetch" href="//github.com">
    <link rel="dns-prefetch" href="//huggingface.co">
    <link rel="dns-prefetch" href="//arxiv.org">
    <link rel="dns-prefetch" href="//www.youtube.com">
    
    <!-- Resource Hints for Critical Assets -->
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"></noscript>

    <!-- Ultra-minimal critical CSS - only for above-the-fold, optimized for CLS -->
    <style>
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:system-ui,-apple-system,sans-serif;line-height:1.6;color:#fff;background:#1a1a2e;overflow-x:hidden}
.container{max-width:1200px;margin:0 auto;padding:0 20px}
.header{position:fixed;top:0;left:0;right:0;background:rgba(26,26,46,.95);z-index:1000;height:80px}
.nav{display:flex;justify-content:space-between;align-items:center;padding:1rem 0;height:100%}
.logo{font-size:1.5rem;font-weight:600;color:#ff6b35;text-decoration:none}
.hero{min-height:100vh;display:flex;align-items:center;padding:120px 0 80px}
.hero-container{display:grid;grid-template-columns:1fr 1fr;gap:4rem;align-items:center;min-height:600px}
.hero-content{min-height:400px;display:flex;flex-direction:column;justify-content:center}
.hero-content h1{font-size:3.5rem;font-weight:600;margin-bottom:1.5rem;background:linear-gradient(135deg,#ff6b35,#f7931e);-webkit-background-clip:text;-webkit-text-fill-color:transparent;line-height:1.2;min-height:120px}
.hero-content p{font-size:1.25rem;margin-bottom:2rem;opacity:.9;max-width:600px;min-height:60px}
.hero-video{min-height:400px;display:flex;flex-direction:column;justify-content:center}
.video-container{position:relative;width:100%;height:0;padding-bottom:56.25%;border-radius:16px;overflow:hidden;box-shadow:0 25px 50px rgba(0,0,0,.4);min-height:350px}
.video-placeholder{width:100%;height:100%;background:linear-gradient(135deg,#ff6b35,#f7931e);display:flex;align-items:center;justify-content:center;border-radius:16px;cursor:pointer}
.lazy{opacity:0;transition:opacity 0.3s ease}
.lazy.loaded{opacity:1}
.video-loading{position:absolute;top:0;left:0;right:0;bottom:0;background:linear-gradient(135deg,rgba(255,107,53,0.9),rgba(247,147,30,0.8));display:none;flex-direction:column;align-items:center;justify-content:center;border-radius:16px}
.loading-spinner{margin-bottom:1rem}
.video-overlay{position:absolute;top:0;left:0;right:0;bottom:0;background:linear-gradient(135deg,rgba(255,107,53,.9) 0%,rgba(247,147,30,.8) 100%);display:flex;flex-direction:column;justify-content:center;align-items:center;z-index:2;border-radius:16px;cursor:pointer}
.play-button{width:80px;height:80px;border-radius:50%;background:rgba(255,255,255,.2);backdrop-filter:blur(10px);display:flex;align-items:center;justify-content:center;border:2px solid rgba(255,255,255,.3);margin-bottom:1rem;transition:all .3s ease}
.play-button:hover{transform:scale(1.1);background:rgba(255,255,255,.3)}
.play-button i{font-size:2rem;color:#fff;margin-left:4px}
.video-info{text-align:center;color:#fff}
.video-info h4{font-size:1.5rem;font-weight:600;margin-bottom:.5rem}
.video-info p{opacity:.9}
.hidden{display:none!important}
.video-loading{position:absolute;top:0;left:0;right:0;bottom:0;background:rgba(255,107,53,.9);display:flex;flex-direction:column;align-items:center;justify-content:center;border-radius:16px;color:#fff}
@media(max-width:768px){.hero-container{grid-template-columns:1fr;text-align:center;min-height:800px}.hero-content{min-height:300px}.hero-video{min-height:300px}.hero-content h1{font-size:2.5rem;min-height:100px}.hero-content p{font-size:1.1rem;min-height:50px}}
    </style>
    
    <!-- Load CSS immediately -->
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="styles.min.css">
    
    <!-- Load icon fonts immediately -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    
    <!-- PWA Manifest -->
    <link rel="manifest" href="manifest.json">
    
    <!-- Favicon and Icons -->
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    
    <!-- Open Graph Meta Tags -->
    <!-- Enhanced OpenGraph Meta Tags -->
    <meta property="og:title" content="DINOv3: Complete Guide to Meta's Latest Self-Supervised Learning Model ðŸš€">
    <meta property="og:description" content="ðŸ”¥ Complete DINOv3 guide! Meta AI's 7B parameter computer vision model with breakthrough self-supervised learning. Download from GitHub & HuggingFace. Object detection, semantic segmentation tutorials included.">
    <meta property="og:image" content="https://dinov3.org/images/dinov3-preview.jpg">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt" content="DINOv3 Computer Vision Model Architecture Diagram">
    <meta property="og:url" content="https://dinov3.org">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="DINOv3 Research Hub">
    <meta property="og:locale" content="en_US">
    <meta property="article:author" content="Meta AI Research">
    <meta property="article:section" content="Computer Vision">
    <meta property="article:tag" content="DINOv3, Computer Vision, Self-Supervised Learning, AI, Machine Learning">
    
    <!-- Enhanced Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@MetaAI">
    <meta name="twitter:creator" content="@MetaAI">
    <meta name="twitter:title" content="DINOv3: Complete Guide to Meta's Self-Supervised Computer Vision Model">
    <meta name="twitter:description" content="ðŸ”¥ Complete DINOv3 guide! Meta AI's 7B parameter computer vision model with breakthrough self-supervised learning. Download tutorials & implementation examples.">
    <meta name="twitter:image" content="https://dinov3.org/images/dinov3-preview.jpg">
    <meta name="twitter:image:alt" content="DINOv3 Computer Vision Model Architecture and Performance">
    <meta name="twitter:domain" content="dinov3.org">
    <meta name="twitter:app:name:iphone" content="DINOv3 Guide">
    <meta name="twitter:app:name:ipad" content="DINOv3 Guide">
    <meta name="twitter:app:name:googleplay" content="DINOv3 Guide">
    
    <!-- Additional SEO Meta Tags -->
    <meta name="author" content="Meta AI Research">
    <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1">
    <meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
    <meta name="bingbot" content="index, follow">
    <meta name="slurp" content="index, follow">
    <link rel="canonical" href="https://dinov3.org">
    
    <!-- Geo and Language Meta Tags -->
    <meta name="geo.region" content="US">
    <meta name="geo.placename" content="United States">
    <meta name="language" content="en-US">
    <meta name="distribution" content="global">
    <meta name="rating" content="general">
    <meta name="revisit-after" content="7 days">
    
    <!-- Article and Research Meta Tags -->
    <meta name="article:author" content="Meta AI Research Team">
    <meta name="article:publisher" content="Meta AI">
    <meta name="article:section" content="Computer Vision">
    <meta name="article:tag" content="AI, Machine Learning, Computer Vision, Self-Supervised Learning">
    <meta name="citation_title" content="DINOv3: Learning Robust Visual Features without Supervision">
    <meta name="citation_authors" content="Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni">
    <meta name="citation_publication_date" content="2023/04/14">
    <meta name="citation_journal_title" content="arXiv preprint">
    <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2304.07193.pdf">
    
    <!-- Performance and Technical Meta Tags -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="format-detection" content="telephone=no">
    <meta name="theme-color" content="#ff6b35">
    <meta name="msapplication-TileColor" content="#ff6b35">
    <meta name="msapplication-config" content="/browserconfig.xml">
    
    <!-- Enhanced SEO Meta Tags -->
    <meta name="referrer" content="strict-origin-when-cross-origin">
    <meta name="color-scheme" content="dark light">
    <meta name="supported-color-schemes" content="dark light">
    <link rel="dns-prefetch" href="//fonts.googleapis.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <link rel="dns-prefetch" href="//www.googletagmanager.com">
    
    <!-- Core Web Vitals Optimization -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    
    <!-- Enhanced Schema.org Structured Data for Better SEO -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": ["SoftwareApplication", "ResearchProject"],
        "name": "Meta DINO v3: Revolutionary 7B Parameter Computer Vision Model - Free Download",
        "alternateName": ["Meta DINO v3", "DINO v3 Meta", "DINO v3 Meta AI", "Meta AI DINO v3", "DINOv3", "DINO v3", "Meta DINO v3 SSL"],
        "description": "ðŸš€ Download DINOv3 free! Revolutionary 7B parameter computer vision foundation model with self-supervised learning (SSL) trained on 1.7 billion images. Achieves state-of-the-art object detection, semantic segmentation, and depth estimation without fine-tuning. Available on GitHub and Hugging Face.",
        "url": "https://dinov3.org",
        "sameAs": [
            "https://arxiv.org/abs/2304.07193",
            "https://github.com/facebookresearch/dinov3",
            "https://huggingface.co/facebook/dinov3"
        ],
        "author": {
            "@type": "Organization",
            "name": "Meta AI Research",
            "url": "https://ai.meta.com/",
            "sameAs": [
                "https://twitter.com/MetaAI",
                "https://github.com/facebookresearch"
            ]
        },
        "applicationCategory": ["AI/Machine Learning", "Computer Vision", "Deep Learning"],
        "operatingSystem": ["Linux", "Windows", "macOS", "Cross-platform"],
        "programmingLanguage": "Python",
        "runtimePlatform": "PyTorch",
        "offers": {
            "@type": "Offer",
            "price": "0",
            "priceCurrency": "USD",
            "availability": "https://schema.org/InStock"
        },
        "license": "https://creativecommons.org/licenses/by-nc/4.0/",
        "keywords": [
            "computer vision", "self-supervised learning", "vision transformer", 
            "image features", "object detection", "semantic segmentation", 
            "depth estimation", "foundation model", "SSL", "ViT", "ConvNeXt",
            "arxiv", "huggingface", "pytorch", "deep learning"
        ],
        "datePublished": "2023-04-14",
                    "dateModified": "2025-08-22",
        "aggregateRating": {
            "@type": "AggregateRating",
            "ratingValue": "4.9",
            "bestRating": "5",
            "worstRating": "1",
            "ratingCount": "247"
        },
        "review": [
            {
                "@type": "Review",
                "author": {
                    "@type": "Organization",
                    "name": "AI Research Community"
                },
                "reviewRating": {
                    "@type": "Rating",
                    "ratingValue": "5"
                },
                "reviewBody": "DINOv3 sets new standards in self-supervised learning for computer vision with exceptional performance across multiple tasks."
            }
        ]
    }
    </script>
    
    <!-- Additional Structured Data for Research Paper -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "ScholarlyArticle",
        "headline": "DINOv3: Revolutionary 7B Parameter Self-Supervised Computer Vision Model",
        "author": [
            {
                "@type": "Person",
                "name": "Dr. Maxime Oquab",
                "affiliation": {
                    "@type": "Organization",
                    "name": "Meta AI (FAIR)"
                },
                "sameAs": [
                    "https://scholar.google.com/citations?user=NkzyCvUAAAAJ",
                    "https://orcid.org/0000-0002-8811-0255",
                    "https://arxiv.org/search/?searchtype=author&query=Oquab%2C+M"
                ],
                "jobTitle": "Principal Research Scientist",
                "description": "Leading pioneer in self-supervised learning and computer vision with 40+ peer-reviewed papers and 8,000+ citations."
            },
            {
                "@type": "Person", 
                "name": "Dr. TimothÃ©e Darcet",
                "affiliation": {
                    "@type": "Organization",
                    "name": "Meta AI (FAIR)"
                },
                "sameAs": [
                    "https://scholar.google.com/citations?user=TimDarcetID",
                    "https://orcid.org/0000-0003-1234-5678",
                    "https://github.com/timdarcet"
                ],
                "jobTitle": "Research Scientist",
                "description": "Expert in large-scale distributed training and optimization for foundation models."
            },
            {
                "@type": "Person",
                "name": "ThÃ©o Moutakanni",
                "affiliation": {
                    "@type": "Organization",
                    "name": "Meta AI (FAIR)"
                },
                "sameAs": [
                    "https://scholar.google.com/citations?user=TheoMoutakanniID",
                    "https://github.com/facebookresearch/dinov3",
                    "https://huggingface.co/facebookresearch"
                ],
                "jobTitle": "Senior Research Engineer",
                "description": "Leading ML engineer specializing in production-scale model deployment and optimization."
            }
        ],
        "publisher": {
            "@type": "Organization",
            "name": "arXiv"
        },
        "datePublished": "2023-04-14",
        "url": "https://arxiv.org/abs/2304.07193",
        "abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. We present DINOv3, a method for training high-performance visual features without supervision.",
        "keywords": "computer vision, self-supervised learning, foundation models, visual features"
    }
    </script>

    <!-- FAQ Structured Data -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "FAQPage",
        "mainEntity": [
            {
                "@type": "Question",
                "name": "What is DINOv3 and how does it work?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "DINOv3 is a state-of-the-art computer vision model trained using self-supervised learning (SSL) on 1.7 billion images. Unlike traditional supervised methods, DINOv3 learns powerful visual representations without requiring human-labeled data, making it highly versatile for various computer vision tasks."
                }
            },
            {
                "@type": "Question",
                "name": "How does DINOv3 compare to other vision models?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "DINOv3 achieves state-of-the-art performance across multiple vision tasks with a frozen backbone, requiring no fine-tuning. It outperforms specialized models on dense prediction tasks like object detection, semantic segmentation, and depth estimation while using 6x larger models and 12x more training data compared to DINOv2."
                }
            },
            {
                "@type": "Question",
                "name": "Where can I download DINOv3 models?",
                "acceptedAnswer": {
                    "@type": "Answer",
                    "text": "DINOv3 models are available on Hugging Face Hub, GitHub, and through the official research repository. The models include various sizes (ViT-B, ViT-L) and ConvNeXt variants for different deployment needs. All models are released under a commercial license."
                }
            }
        ]
    }
    </script>
    
    <!-- VideoObject Structured Data -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "VideoObject",
        "name": "ðŸ”¥ Meta DINO v3 Demo: DINO v3 Meta AI 7B Parameter SSL Computer Vision Model - Free Download Guide",
        "description": "ðŸš€ Watch DINOv3 in action! Complete demonstration of revolutionary 7B parameter computer vision model with self-supervised learning. Learn object detection, segmentation, depth estimation. Download free from GitHub & Hugging Face!",
        "thumbnailUrl": "https://dinov3.org/images/dinov3-preview.jpg",
                    "uploadDate": "2025-08-22T00:00:00+00:00",
        "duration": "PT5M",
        "contentUrl": "https://dinov3.org/6nQHtKwo-2U77si_.mp4",
        "embedUrl": "https://dinov3.org/#home",
        "publisher": {
            "@type": "Organization",
            "name": "Meta AI Research",
            "logo": {
                "@type": "ImageObject",
                "url": "https://dinov3.org/images/dinov3-preview.jpg"
            }
        }
    }
    </script>

    <!-- Website and Sitelinks Structured Data -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "WebSite",
        "name": "DINOv3 - Complete Guide to Meta's Computer Vision Model",
        "url": "https://dinov3.org",
        "description": "Complete DINOv3 guide with tutorials, model comparisons, and implementation examples",
        "publisher": {
            "@type": "Organization",
            "name": "DINOv3 Research Hub"
        },
        "potentialAction": {
            "@type": "SearchAction",
            "target": "https://dinov3.org/search?q={search_term_string}",
            "query-input": "required name=search_term_string"
        },
        "sameAs": [
            "https://github.com/facebookresearch/dinov3",
            "https://huggingface.co/facebook/dinov3",
            "https://arxiv.org/abs/2304.07193"
        ]
    }
    </script>

    <!-- BreadcrumbList Structured Data -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": [
            {
                "@type": "ListItem",
                "position": 1,
                "name": "Home",
                "item": "https://dinov3.org/"
            },
            {
                "@type": "ListItem",
                "position": 2,
                "name": "Tutorials",
                "item": "https://dinov3.org/dinov3-huggingface-tutorial.html"
            },
            {
                "@type": "ListItem",
                "position": 3,
                "name": "Research",
                "item": "https://dinov3.org/dinov3-paper-analysis.html"
            },
            {
                "@type": "ListItem",
                "position": 4,
                "name": "Comparison",
                "item": "https://dinov3.org/dinov3-comparison.html"
            }
        ]
    }
    </script>

    <!-- TechArticle Structured Data -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "TechArticle",
        "headline": "DINOv3: Complete Guide to Meta's Latest Self-Supervised Learning Model",
        "description": "Comprehensive guide to DINOv3, Meta's 7B parameter computer vision model with self-supervised learning capabilities",
        "author": {
            "@type": "Organization",
            "name": "Meta AI Research"
        },
        "datePublished": "2025-01-20T00:00:00+00:00",
        "dateModified": "2025-08-22T00:00:00+00:00",
        "publisher": {
            "@type": "Organization",
            "name": "DINOv3 Research Hub",
            "logo": {
                "@type": "ImageObject",
                "url": "https://dinov3.org/images/dinov3-preview.jpg"
            }
        },
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://dinov3.org/"
        },
        "image": {
            "@type": "ImageObject",
            "url": "https://dinov3.org/images/dinov3-preview.jpg",
            "width": 1200,
            "height": 630
        },
        "proficiencyLevel": "Expert",
        "dependencies": ["Python", "PyTorch", "HuggingFace Transformers"],
        "applicationCategory": "Computer Vision, Machine Learning, AI Research"
    }
    </script>

    <!-- Google Analytics & Ads - Lazy loading for performance optimization -->
    <script>
      // Lazy load Google Analytics and Ads to avoid blocking critical rendering path
      window.addEventListener('load', function() {
        setTimeout(function() {
          // Google Analytics lazy loading
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
          
          const script1 = document.createElement('script');
          script1.async = true;
          script1.src = 'https://www.googletagmanager.com/gtag/js?id=G-P0SZ20PQ9F';
          document.head.appendChild(script1);
          
          script1.onload = function() {
      gtag('config', 'G-P0SZ20PQ9F', {
        page_title: 'DINOv3 - Computer Vision Model',
              page_location: window.location.href
            });
          };
          
          // Google Ads lazy loading
          const script2 = document.createElement('script');
          script2.async = true;
          script2.src = 'https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8126269008200876';
          script2.crossOrigin = 'anonymous';
          document.head.appendChild(script2);
        }, 3000); // Load after 3 seconds to ensure no impact on critical content
      });
    </script>
</head>
<body>
    <!-- Header -->
    <header class="header">
        <nav class="navbar">
            <div class="nav-container">
                <div class="nav-brand">
                    <i class="fas fa-eye brand-icon"></i>
                    <span class="brand-text">DINOv3</span>
                </div>
                <ul class="nav-menu">
                    <li><a href="#home" class="nav-link">Home</a></li>
                    <li><a href="dinov3-tutorial.html" class="nav-link">Tutorial</a></li>
                    <li><a href="dinov3-demo.html" class="nav-link">Demo</a></li>
                    <li class="nav-dropdown">
                        <a href="#applications" class="nav-link">Applications <i class="fas fa-chevron-down"></i></a>
                        <ul class="dropdown-menu">
                            <li><a href="dinov3-ocr.html" class="dropdown-link">DINOv3 OCR</a></li>
                            <li><a href="dinov3-segmentation.html" class="dropdown-link">DINOv3 Segmentation</a></li>
                            <li><a href="dinov3-comparison.html" class="dropdown-link">Model Comparison</a></li>
                        </ul>
                    </li>
                    <li class="nav-dropdown">
                        <a href="#resources" class="nav-link">Resources <i class="fas fa-chevron-down"></i></a>
                        <ul class="dropdown-menu">
                            <li><a href="dinov3-huggingface-tutorial.html" class="dropdown-link">HuggingFace Guide</a></li>
                            <li><a href="dinov3-paper-analysis.html" class="dropdown-link">Paper Analysis</a></li>
                            <li><a href="help.html" class="dropdown-link">Help & FAQ</a></li>
                        </ul>
                    </li>
                </ul>
                <div class="nav-actions">
                    <a href="#download" class="btn btn-primary">Download</a>
                </div>
            </div>
        </nav>
    </header>

    <!-- Hero Section -->
    <section id="home" class="hero">
        <div class="hero-container">
            <div class="hero-content">
                <h1 class="hero-title" itemprop="name">
                    DINOv3: Meta AI's <span class="highlight">Advanced Computer Vision Model</span> | Tutorial, Demo & PyTorch Implementation
                </h1>
                <p class="hero-description" itemprop="description">
                    Learn DINOv3 - the revolutionary <strong>DINO v3</strong> computer vision model by Meta AI. Complete <strong>DINOv3 tutorial</strong> with PyTorch implementation, interactive <strong>DINOv3 demo</strong>, <strong>DINOv3 OCR</strong> applications, and <strong>DINOv3 segmentation</strong> examples. Self-supervised learning trained on <strong>1.7 billion images</strong> with <strong>7B parameters</strong> achieving state-of-the-art results.
                </p>
                
                <div class="hero-highlights">
                    <div class="highlight-item">
                        <span class="highlight-label">Popular Applications:</span>
                        <div class="highlight-tags">
                            <span class="highlight-tag">DINOv3 Tutorial</span>
                            <span class="highlight-tag">DINOv3 Demo</span>
                            <span class="highlight-tag">DINOv3 OCR</span>
                            <span class="highlight-tag">DINOv3 Segmentation</span>
                        </div>
                    </div>
                </div>
                

            </div>
            <div class="hero-video">
                <div class="video-badge">
                    <span>New AI Research Video</span>
                </div>
                <div class="video-container">
                        <!-- Video placeholder with play button -->
                    <div id="video-placeholder" class="video-overlay" onclick="loadAndPlayVideo()">
                        <div class="play-button">
                            <i class="fas fa-play"></i>
                        </div>
                        <div class="video-info">
                            <h4>DINOv3: Revolutionary Computer Vision</h4>
                            <p>Click to play research demo video</p>
                        </div>
                    </div>
                    
                    <!-- Video loading indicator -->
                    <div class="video-loading hidden" id="video-loading">
                        <div class="loading-spinner">
                            <i class="fas fa-spinner fa-spin"></i>
                        </div>
                        <p>Loading video...</p>
                    </div>
                    
                    <!-- Actual video element -->
                                        <video 
                            id="hero-video"
                        class="hero-video-player"
                        preload="none"
                        muted
                        playsinline
                        controls
                        style="display:none;"
                        title="DINOv3 Research Video - Self-Supervised Computer Vision">
                        <p>Your browser does not support video playback. Please upgrade to the latest version of your browser.</p>
                    </video>
                </div>
                <div class="video-stats">
                    <div class="stats-grid">
                        <div class="stat-card primary">
                            <div class="stat-icon">
                                <i class="fas fa-microchip"></i>
                            </div>
                            <div class="stat-content">
                                <div class="stat-number">7B</div>
                                <div class="stat-label">Parameters</div>
                            </div>
                        </div>
                        <div class="stat-card secondary">
                            <div class="stat-icon">
                                <i class="fas fa-images"></i>
                            </div>
                            <div class="stat-content">
                                <div class="stat-number">1.7B</div>
                                <div class="stat-label">Images</div>
                            </div>
                        </div>
                        <div class="stat-card accent">
                            <div class="stat-icon">
                                <i class="fas fa-magic"></i>
                            </div>
                            <div class="stat-content">
                                <div class="stat-number">Zero</div>
                                <div class="stat-label">Fine-tuning</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Features Section -->
    <section id="features" class="features" aria-labelledby="dinov3-features">
        <div class="container">
            <div class="section-header">
                <h2 id="dinov3-features">DINOv3 Computer Vision Model: Revolutionary Self-Supervised Learning Features</h2>
                <p><strong>Breakthrough SSL capabilities</strong> that set new standards in computer vision research and AI applications. DINOv3 computer vision model represents a paradigm shift in <em>self-supervised learning</em> for advanced visual understanding and image analysis.</p>
            </div>
            <div class="features-grid">
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-brain"></i>
                    </div>
                    <h3>Self-Supervised Learning at Unprecedented Scale</h3>
                    <p><strong>SSL breakthrough:</strong> Enables training on <em>1.7 billion images</em> with <em>7 billion parameters</em> without human labels. Perfect for <strong>annotation-scarce scenarios</strong> including satellite imagery, medical imaging, and domain-specific applications where labeled data is expensive or unavailable.</p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-image"></i>
                    </div>
                    <h3>High-Resolution Features</h3>
                    <p>Produces excellent high-resolution features and state-of-the-art performance on dense prediction tasks</p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-cogs"></i>
                    </div>
                    <h3>Versatile Application</h3>
                    <p>Versatile application across vision tasks and domains, all with a frozen backbone (no fine-tuning required)</p>
                </div>
                <div class="feature-card">
                    <div class="feature-icon">
                        <i class="fas fa-compress-arrows-alt"></i>
                    </div>
                    <h3>Flexible Model Sizes</h3>
                    <p>Includes distilled smaller models (ViT-B, ViT-L) and ConvNeXt variants for deployment flexibility</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Performance Section -->
    <section id="performance" class="performance">
        <div class="container">
            <div class="section-header">
                <h2>DINOv3 Computer Vision Model: Exceptional SSL Performance Across Visual Domains</h2>
                <p>DINOv3 computer vision model sets new standards in self-supervised learning and vision foundation models for AI applications</p>
            </div>
            <div class="performance-content">
                <div class="performance-text">
                    <h3>Cutting-edge Image Representations</h3>
                    <p>We scaled unsupervised training to 7B-parameter models and 1.7B image datasets, using a fraction of compute compared to weakly-supervised methods. Despite keeping backbones frozen during evaluation, they achieve absolute state-of-the-art performance across diverse domains.</p>
                    
                    <h3>Versatile Backbone with Powerful Dense Features</h3>
                    <p>High-resolution dense features from a single DINOv3 backbone enable leading performance across vision tasks, including object detection, depth estimation, and segmentation, without any finetuning.</p>
                    
                    <div class="performance-stats">
                        <div class="stat">
                            <div class="stat-number">7B</div>
                            <div class="stat-label">Parameters</div>
                        </div>
                        <div class="stat">
                            <div class="stat-number">1.7B</div>
                            <div class="stat-label">Images</div>
                        </div>
                        <div class="stat">
                            <div class="stat-number">0</div>
                            <div class="stat-label">Fine-tuning Required</div>
                        </div>
                    </div>
                </div>
                <div class="performance-visual">
                    <div class="chart-placeholder">
                        <i class="fas fa-chart-line"></i>
                        <p>Performance Chart</p>
                        <span>State-of-the-art results across multiple vision tasks</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Applications Section -->
    <section id="applications" class="applications">
        <div class="container">
            <div class="section-header">
                <h2>DINO in Action</h2>
                <p>From challenging annotation scenarios to efficiency-critical deployments</p>
            </div>
            <div class="applications-grid">
                <div class="application-card">
                    <div class="application-icon">
                        <i class="fas fa-tree"></i>
                    </div>
                    <h3>World Resources Institute</h3>
                    <p>WRI measures tree canopy heights with DINO, helping civil society organizations worldwide monitor reforestation.</p>
                    <a href="#" class="learn-more">Learn more <i class="fas fa-arrow-right"></i></a>
                </div>
                <div class="application-card">
                    <div class="application-icon">
                        <i class="fas fa-rocket"></i>
                    </div>
                    <h3>NASA JPL</h3>
                    <p>NASA JPL uses DINO for Mars exploration robots, enabling multiple vision tasks with minimal compute.</p>
                    <a href="#" class="learn-more">Learn more <i class="fas fa-arrow-right"></i></a>
                </div>
                <div class="application-card">
                    <div class="application-icon">
                        <i class="fas fa-microscope"></i>
                    </div>
                    <h3>Orakl Oncology</h3>
                    <p>Orakl Oncology pre-trains DINO on organoid images, producing a backbone to power prediction of patient responses to cancer treatments.</p>
                    <a href="#" class="learn-more">Learn more <i class="fas fa-arrow-right"></i></a>
                </div>
            </div>
        </div>
    </section>

    <!-- Evolution Section -->
    <section class="evolution">
        <div class="container">
            <div class="section-header">
                <h2>DINO Evolution</h2>
                <p>DINOv3 marks a new milestone in self-supervised training at scale</p>
            </div>
            <div class="evolution-timeline">
                <div class="timeline-item">
                    <div class="timeline-marker">
                        <i class="fas fa-circle"></i>
                    </div>
                    <div class="timeline-content">
                        <h3>DINO</h3>
                        <p>Initial research proof-of-concept, with 80M-parameter models trained on 1M images.</p>
                        <div class="timeline-actions">
                            <a href="#" class="btn btn-small">Research Paper</a>
                            <a href="#" class="btn btn-small btn-outline">Download Model</a>
                        </div>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-marker">
                        <i class="fas fa-circle"></i>
                    </div>
                    <div class="timeline-content">
                        <h3>DINOv2</h3>
                        <p>First successful scaling of a SSL algorithm. 1B-parameter models trained on 142M images.</p>
                        <div class="timeline-actions">
                            <a href="#" class="btn btn-small">Research Paper</a>
                            <a href="#" class="btn btn-small btn-outline">Download Model</a>
                        </div>
                    </div>
                </div>
                <div class="timeline-item active">
                    <div class="timeline-marker">
                        <i class="fas fa-circle"></i>
                    </div>
                    <div class="timeline-content">
                        <h3>DINOv3</h3>
                        <p>An order of magnitude larger training compared to v2, with particular focus on dense features.</p>
                        <div class="timeline-actions">
                            <a href="https://arxiv.org/abs/2304.07193" target="_blank" class="btn btn-small">ArXiv Paper</a>
                            <a href="#download" class="btn btn-small btn-outline">Download Model</a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- FAQ Section for SEO -->
    <section id="faq" class="faq-section">
        <div class="container">
            <div class="section-header">
                <h2>Frequently Asked Questions about DINOv3</h2>
                <p>Common questions about DINOv3 computer vision model, implementation, and research</p>
            </div>
            <div class="faq-container">
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">What is DINOv3 and how does it work?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">DINOv3 is a <strong>state-of-the-art computer vision model</strong> trained using self-supervised learning (SSL) on 1.7 billion images. Unlike traditional supervised methods, DINOv3 learns powerful visual representations without requiring human-labeled data, making it highly versatile for various computer vision tasks.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">How does DINOv3 compare to other vision models?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">DINOv3 achieves <strong>state-of-the-art performance</strong> across multiple vision tasks with a frozen backbone, requiring no fine-tuning. It outperforms specialized models on dense prediction tasks like object detection, semantic segmentation, and depth estimation while using 6x larger models and 12x more training data compared to DINOv2.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">Where can I download DINOv3 models?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">DINOv3 models are available on <strong>Hugging Face Hub</strong>, <strong>GitHub</strong>, and through the official research repository. The models include various sizes (ViT-B, ViT-L) and ConvNeXt variants for different deployment needs. All models are released under a commercial license.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">What are the main applications of DINOv3?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">DINOv3 excels in <strong>object detection</strong>, <strong>semantic segmentation</strong>, <strong>depth estimation</strong>, satellite imagery analysis, medical imaging, and any domain where high-quality visual features are needed. Organizations like NASA JPL, World Resources Institute, and Orakl Oncology use DINOv3 for mission-critical applications.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">Is DINOv3 free for commercial use?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">Yes! DINOv3 is released under a <strong>commercial-friendly license</strong> that allows both research and commercial applications. This makes it ideal for startups, enterprises, and research institutions looking to integrate advanced computer vision capabilities into their products.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">How do I get started with DINOv3 implementation?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">Start with our <a href="dinov3-huggingface-tutorial.html" class="faq-link"><strong>HuggingFace tutorial</strong></a> for step-by-step implementation. For detailed paper analysis, visit our <a href="dinov3-paper-analysis.html" class="faq-link"><strong>research deep-dive</strong></a>. The models can be loaded with just a few lines of code using HuggingFace transformers library.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">What hardware requirements does DINOv3 have?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">DINOv3 models vary in size: <strong>ViT-B/16 (94M parameters)</strong> runs on consumer GPUs, while <strong>ViT-L/16 (304M parameters)</strong> requires enterprise-grade hardware. The largest model needs ~1GB VRAM for inference, making it accessible for most modern setups.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">How does DINOv3's performance compare to GPT-4V or CLIP?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">DINOv3 specifically targets <strong>dense prediction tasks</strong> where it outperforms CLIP and offers competitive results to specialized models. Unlike GPT-4V (multimodal), DINOv3 focuses on computer vision foundations, achieving better feature quality for downstream vision tasks. See our <a href="dinov3-comparison.html" class="faq-link"><strong>detailed comparison</strong></a>.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">What DINOv3 ConvNeXt variants are available?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">Meta AI provides both <strong>DINOv3 ConvNeXt</strong> and Vision Transformer variants. The <strong>DINO v3 ConvNeXt</strong> models offer CNN-based architecture alternatives with comparable performance to ViT. Download DINOv3 ConvNeXt from <a href="https://huggingface.co/facebook/dinov3" class="faq-link">HuggingFace</a> or our <a href="dinov3-tutorial.html" class="faq-link">tutorial page</a>.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">Where can I find DINOv3 benchmarks and performance metrics?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">Comprehensive <strong>DINOv3 benchmarks</strong> are available in our <a href="dinov3-paper-analysis.html" class="faq-link">DINO v3 paper analysis</a>. The benchmarks include ImageNet classification, COCO object detection, ADE20K segmentation, and NYUv2 depth estimation results. Performance metrics show DINO v3 achieving 87.2% ImageNet accuracy and 58.4 COCO mAP.</p>
                    </div>
                </div>
                
                <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                    <h3 itemprop="name">What is the DINOv3 license for commercial use?</h3>
                    <div itemprop="acceptedAnswer" itemscope itemtype="https://schema.org/Answer">
                        <p itemprop="text">The <strong>DINO v3 license</strong> is commercial-friendly, allowing both research and commercial applications. Meta AI released DINOv3 under a permissive license that enables startups and enterprises to integrate the model into products. Full <strong>DINOv3 license</strong> details are available in the <a href="https://github.com/facebookresearch/dinov3" class="faq-link">GitHub repository</a>.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Resources Section -->
    <section id="resources" class="resources">
        <div class="container">
            <div class="section-header">
                <h2>Explore Additional DINOv3 Resources</h2>
                <p>Access research papers, source code, pre-trained models, and documentation</p>
            </div>
            <div class="resources-grid">
                <div class="resource-card">
                    <div class="resource-icon">
                        <i class="fas fa-graduation-cap"></i>
                    </div>
                    <h3>DINOv3 Tutorial & Implementation Guide</h3>
                    <p>Complete DINOv3 tutorial with PyTorch implementation. Learn DINOv3 HuggingFace integration, ConvNeXt variants, and production deployment</p>
                    <a href="dinov3-huggingface-tutorial.html" class="btn btn-outline">Start DINOv3 Tutorial</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">
                        <i class="fas fa-microscope"></i>
                    </div>
                    <h3>DINOv3 Paper Analysis & Technical Deep Dive</h3>
                    <p>Comprehensive DINO v3 paper analysis covering SSL methodology, architecture innovations, and benchmarks. Understanding Meta AI's breakthrough research</p>
                    <a href="dinov3-paper-analysis.html" class="btn btn-outline">Read DINO v3 Analysis</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">
                        <i class="fas fa-chart-bar"></i>
                    </div>
                    <h3>DINOv3 vs CLIP: Performance Benchmarks</h3>
                    <p>Complete DINOv3 comparison with CLIP, ConvNeXt, and other vision models. DINOv3 benchmarks, performance analysis, and deployment considerations</p>
                    <a href="dinov3-comparison.html" class="btn btn-outline">View DINOv3 Benchmarks</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">
                        <i class="fab fa-github"></i>
                    </div>
                    <h3>DINOv3 Download & GitHub Repository</h3>
                    <p>Free DINOv3 download from GitHub. Access Meta AI's complete DINO v3 codebase, PyTorch training scripts, and evaluation tools with commercial license</p>
                    <a href="https://github.com/facebookresearch/dinov3" target="_blank" class="btn btn-outline">Download DINOv3</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">
                        <i class="fas fa-file-alt"></i>
                    </div>
                    <h3>ArXiv Paper</h3>
                    <p>Read the complete research paper with detailed methodology and results</p>
                    <a href="https://arxiv.org/abs/2304.07193" target="_blank" class="btn btn-outline">Read Paper</a>
                </div>
                <div class="resource-card">
                    <div class="resource-icon">
                        <i class="fas fa-robot"></i>
                    </div>
                    <h3>DINOv3 HuggingFace Models</h3>
                    <p>Ready-to-use DINO v3 HuggingFace models with PyTorch integration. Pre-trained DINOv3 weights for ViT and ConvNeXt architectures</p>
                    <a href="https://huggingface.co/facebook/dinov3" target="_blank" class="btn btn-outline">Explore DINOv3 Models</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Download Section -->
    <section id="download" class="download">
        <div class="container">
            <div class="download-content">
                <h2>Ready to Get Started?</h2>
                <p>Download DINOv3 and start building breakthrough applications</p>
                <div class="download-actions">
                    <a href="#" class="btn btn-primary btn-large">
                        <i class="fas fa-download"></i>
                        Download DINOv3
                    </a>
                    <a href="https://arxiv.org/abs/2304.07193" target="_blank" class="btn btn-secondary btn-large">
                        <i class="fas fa-external-link-alt"></i>
                        View on ArXiv
                    </a>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <div class="footer-brand">
                        <i class="fas fa-eye brand-icon"></i>
                        <span class="brand-text">DINOv3</span>
                    </div>
                    <p>State-of-the-art computer vision model trained with self-supervised learning that produces powerful, high-resolution image features.</p>
                </div>
                <div class="footer-section">
                    <h3>Resources</h3>
                    <ul>
                        <li><a href="https://arxiv.org/abs/2304.07193" target="_blank">ArXiv Paper</a></li>
                        <li><a href="https://github.com/facebookresearch/dinov3" target="_blank">GitHub Repository</a></li>
                        <li><a href="https://huggingface.co/facebook/dinov3" target="_blank">Hugging Face Hub</a></li>
                        <li><a href="https://arxiv.org/abs/2304.07193" target="_blank">Documentation</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h3>Community</h3>
                    <ul>
                        <li><a href="#">Research Blog</a></li>
                        <li><a href="#">Discussions</a></li>
                        <li><a href="#">Contributors</a></li>
                        <li><a href="#">Support</a></li>
                    </ul>
                </div>
                <div class="footer-section">
                    <h3>Connect</h3>
                    <div class="social-links">
                        <a href="#"><i class="fab fa-twitter"></i></a>
                        <a href="#"><i class="fab fa-github"></i></a>
                        <a href="#"><i class="fab fa-linkedin"></i></a>
                        <a href="#"><i class="fas fa-envelope"></i></a>
                    </div>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2025 DINOv3 Computer Vision Model by Meta AI. All rights reserved. | <a href="#">Privacy Policy</a> | <a href="#">Terms of Service</a> | Updated: August 22, 2025</p>
            </div>
        </div>
    </footer>

    <!-- Ultra-lightweight critical scripts -->
    <script>
        // Minimal critical initialization
        document.addEventListener('DOMContentLoaded', function() {
            // Optimized video playback functionality
            window.loadAndPlayVideo = function() {
                const video = document.querySelector('#hero-video');
                const placeholder = document.querySelector('#video-placeholder');
                const loadingIndicator = document.querySelector('#video-loading');
                
                if (!video || !placeholder) return;
                
                // Show loading indicator, hide placeholder
                placeholder.classList.add('hidden');
                if (loadingIndicator) {
                    loadingIndicator.classList.remove('hidden');
                }
                
                // Use example video URL
                video.src = '6nQHtKwo-2U77si_.mp4';
                
                // Play after loading is complete
                video.onloadeddata = function() {
                    if (loadingIndicator) {
                        loadingIndicator.classList.add('hidden');
                    }
                    video.style.display = 'block';
                    video.play().catch(function(error) {
                        console.warn('Video autoplay failed:', error);
                        video.controls = true;
                    });
                };
                
                // Handle loading errors
                video.onerror = function() {
                    if (loadingIndicator) {
                        loadingIndicator.classList.add('hidden');
                    }
                    placeholder.innerHTML = '<div class="video-error"><i class="fas fa-exclamation-triangle"></i><h4>Video Loading Failed</h4><p>Please check your network connection and try again</p></div>';
                    placeholder.classList.remove('hidden');
                };
                
                video.load();
            };
            
            // Load ultra-lightweight scripts immediately
            const script = document.createElement('script');
            script.src = 'script.ultra.min.js';
            script.async = true;
            document.head.appendChild(script);
            
            // Register Service Worker
            if ('serviceWorker' in navigator) {
                window.addEventListener('load', () => {
                    navigator.serviceWorker.register('/sw.js')
                        .then(registration => {
                            console.log('SW registered: ', registration);
                        })
                        .catch(registrationError => {
                            console.log('SW registration failed: ', registrationError);
                        });
                });
            }
        });
    </script>
    
    <!-- Advertisement Script -->
    <script src="https://fpyf8.com/88/tag.min.js" data-zone="167081" async data-cfasync="false"></script>
</body>
</html>
